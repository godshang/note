(window.webpackJsonp=window.webpackJsonp||[]).push([[69],{497:function(a,e,t){"use strict";t.r(e);var n=t(27),i=Object(n.a)({},(function(){var a=this,e=a._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[e("h1",{attrs:{id:"flink常用api"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#flink常用api"}},[a._v("#")]),a._v(" Flink常用API")]),a._v(" "),e("h2",{attrs:{id:"flink-api的抽象级别分析"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#flink-api的抽象级别分析"}},[a._v("#")]),a._v(" Flink API的抽象级别分析")]),a._v(" "),e("p",[a._v("Flink中提供了4种不同层次的API，每种API在简洁和易用之间有自己的权衡，适用于不同的场景。目前其中的3种API用得比较多，下面自下向上介绍这4种API。")]),a._v(" "),e("ul",[e("li",[a._v("低级API：提供了对时间和状态的细粒度控制，简洁性和易用性较差，主要应用在对一些复杂事件的处理逻辑上。")]),a._v(" "),e("li",[a._v("核心API：主要提供了针对流数据和离线数据的处理，对低级API进行了一些封装，提供了filter、sum、max、min等高级函数，简单且易用，所以在工作中应用比较广泛。")]),a._v(" "),e("li",[a._v("Table API：一般与DataSet或者DataStream紧密关联，首先通过一个DataSet或DataStream创建出一个Table；然后用类似于filter、join或者select关系型转化操作来转化为一个新的Table对象；最后将一个Table对象转回一个DataSet或DataStream。与SQL不同的是，Table API的查询不是一个指定的SQL字符串，而是调用指定的API方法。")]),a._v(" "),e("li",[a._v("SQL：Flink的SQL集成是基于Apache Calcite的，Apache Calcite实现了标准的SQL，使用起来比其他API更加灵活，因为可以直接使用SQL语句。Table API和SQL可以很容易地结合在一块使用，它们都返回Table对象。")])]),a._v(" "),e("h2",{attrs:{id:"flink-datastream的常用api"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#flink-datastream的常用api"}},[a._v("#")]),a._v(" Flink DataStream的常用API")]),a._v(" "),e("p",[a._v("DataStream API主要分为3块：DataSource、Transformation、Sink。")]),a._v(" "),e("ul",[e("li",[a._v("DataSource是程序的数据源输入，可以通过StreamExecutionEnvironment.addSource(sourceFunction)为程序添加一个数据源。")]),a._v(" "),e("li",[a._v("Transformation是具体的操作，它对一个或多个输入数据源进行计算处理，比如Map、FlatMap和Filter等操作。")]),a._v(" "),e("li",[a._v("Sink是程序的输出，它可以把Transformation处理之后的数据输出到指定的存储介质中。")])]),a._v(" "),e("h3",{attrs:{id:"datasource"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#datasource"}},[a._v("#")]),a._v(" DataSource")]),a._v(" "),e("p",[a._v("Flink针对DataStream提供了大量的已经实现的DataSource（数据源）接口，比如下面4种。")]),a._v(" "),e("ol",[e("li",[a._v("基于文件：读取文本文件，文件遵循TextInputFormat逐行读取规则并返回。")])]),a._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("readTextFile(path)\n")])])]),e("ol",{attrs:{start:"2"}},[e("li",[a._v("基于Socket：从Socket中读取数据，元素可以通过一个分隔符分开。")])]),a._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("socketTextStream\n")])])]),e("ol",{attrs:{start:"3"}},[e("li",[a._v("基于集合：通过Java的Collection集合创建一个数据流，集合中的所有元素必须是相同类型的。")])]),a._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("fromCollection(Collection)\n")])])]),e("ol",{attrs:{start:"4"}},[e("li",[a._v("自定义输入")])]),a._v(" "),e("p",[a._v("addSource可以实现读取第三方数据源的数据。")]),a._v(" "),e("p",[a._v("也可以自定义数据源，有两种方式实现。")]),a._v(" "),e("ul",[e("li",[a._v("通过实现SourceFunction接口来自定义无并行度（也就是并行度只能为1）的数 据源。")]),a._v(" "),e("li",[a._v("通过实现ParallelSourceFunction 接口或者继承RichParallelSourceFunction 来自定义有并行度的数据源。")])]),a._v(" "),e("h3",{attrs:{id:"transformation"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#transformation"}},[a._v("#")]),a._v(" Transformation")]),a._v(" "),e("p",[a._v("Flink针对DataStream提供了大量的已经实现的算子。")]),a._v(" "),e("ul",[e("li",[a._v("Map：输入一个元素，然后返回一个元素，中间可以进行清洗转换等操作。")]),a._v(" "),e("li",[a._v("FlatMap：输入一个元素，可以返回零个、一个或者多个元素。")]),a._v(" "),e("li",[a._v("Filter：过滤函数，对传入的数据进行判断，符合条件的数据会被留下。")]),a._v(" "),e("li",[a._v("KeyBy：根据指定的Key进行分组，Key相同的数据会进入同一个分区。")]),a._v(" "),e("li",[a._v("Reduce：对数据进行聚合操作，结合当前元素和上一次Reduce返回的值进行聚合操作，然后返回一个新的值。")]),a._v(" "),e("li",[a._v("Aggregations：sum()、min()、max()等。")]),a._v(" "),e("li",[a._v("Union：合并多个流，新的流会包含所有流中的数据，但是Union有一个限制，就是所有合并的流类型必须是一致的。")]),a._v(" "),e("li",[a._v("Connect：和Union类似，但是只能连接两个流，两个流的数据类型可以不同，会对两个流中的数据应用不同的处理方法。")]),a._v(" "),e("li",[a._v("coMap和coFlatMap：在ConnectedStream中需要使用这种函数，类似于Map和flatMap。")]),a._v(" "),e("li",[a._v("Split：根据规则把一个数据流切分为多个流。")]),a._v(" "),e("li",[a._v("Select：和Split配合使用，选择切分后的流。")])]),a._v(" "),e("p",[a._v("另外，Flink针对DataStream提供了一些数据分区规则，具体如下。")]),a._v(" "),e("ul",[e("li",[a._v("Random partitioning：随机分区。")])]),a._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("DataStream.shuffle()\n")])])]),e("ul",[e("li",[a._v("Rebalancing：对数据集进行再平衡、重分区和消除数据倾斜。")])]),a._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("DataStream.rebalance()\n")])])]),e("ul",[e("li",[a._v("Rescaling：重新调节。")])]),a._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("DataStream.rescale()\n")])])]),e("p",[a._v("Rescaling与Rebalancing的区别为Rebalancing会产生全量重分区，而Rescaling不会。")]),a._v(" "),e("ul",[e("li",[a._v("Custom partitioning：自定义分区。")])]),a._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('DataStream.partitionCustom(partitioner, "someKey")\n或\nDataStream.partitionCustom(partitioner, 0)\n')])])]),e("h3",{attrs:{id:"sink"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#sink"}},[a._v("#")]),a._v(" Sink")]),a._v(" "),e("p",[a._v("Flink针对DataStream提供了大量的已经实现的数据目的地（Sink），具体如下所示。")]),a._v(" "),e("ul",[e("li",[a._v("writeAsText()：将元素以字符串形式逐行写入，这些字符串通过调用每个元素的toString()方法来获取。")]),a._v(" "),e("li",[a._v("print() / printToErr()：打印每个元素的toString()方法的值到标准输出或者标准错误输出流中。")]),a._v(" "),e("li",[a._v("自定义输出：addSink可以实现把数据输出到第三方存储介质中。")])]),a._v(" "),e("h2",{attrs:{id:"flink-dataset的常用api分析"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#flink-dataset的常用api分析"}},[a._v("#")]),a._v(" Flink DataSet的常用API分析")]),a._v(" "),e("p",[a._v("DataSet API主要可以分为3块来分析：DataSource、Transformation和Sink。")]),a._v(" "),e("ul",[e("li",[a._v("DataSource是程序的数据源输入。")]),a._v(" "),e("li",[a._v("Transformation是具体的操作，它对一个或多个输入数据源进行计算处理，比如Map、FlatMap、Filter等操作。")]),a._v(" "),e("li",[a._v("Sink是程序的输出，它可以把Transformation处理之后的数据输出到指定的存储介质中。")])]),a._v(" "),e("p",[a._v("系统提供了一批内置的Connector，它们会提供对应的Sink支持。")]),a._v(" "),e("p",[a._v("也可以自定义Sink，有两种实现方式：")]),a._v(" "),e("ul",[e("li",[a._v("实现SinkFunction接口")]),a._v(" "),e("li",[a._v("继承RichSinkFunction类")])]),a._v(" "),e("h3",{attrs:{id:"datasource-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#datasource-2"}},[a._v("#")]),a._v(" DataSource")]),a._v(" "),e("p",[a._v("对DataSet批处理而言，较频繁的操作是读取HDFS中的文件数据，因此这里主要介绍两个DataSource组件。")]),a._v(" "),e("ol",[e("li",[a._v("基于集合：主要是为了方便测试使用。")])]),a._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("fromCollection(Collection)\n")])])]),e("ol",{attrs:{start:"2"}},[e("li",[a._v("基于文件：基于HDFS中的数据进行计算分析。")])]),a._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("readTextFile(path)\n")])])]),e("h3",{attrs:{id:"transformation-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#transformation-2"}},[a._v("#")]),a._v(" Transformation")]),a._v(" "),e("p",[a._v("Flink针对DataSet提供了大量的已经实现的算子。")]),a._v(" "),e("ul",[e("li",[a._v("Map：输入一个元素，然后返回一个元素，中间可以进行清洗转换等操作。")]),a._v(" "),e("li",[a._v("FlatMap：输入一个元素，可以返回零个、一个或者多个元素。")]),a._v(" "),e("li",[a._v("MapPartition：类似Map，一次处理一个分区的数据（如果在进行Map处理的时候需要获取第三方资源连接，建议使用MapPartition）。")]),a._v(" "),e("li",[a._v("Filter：过滤函数，对传入的数据进行判断，符合条件的数据会被留下。")]),a._v(" "),e("li",[a._v("Reduce：对数据进行聚合操作，结合当前元素和上一次Reduce返回的值进行聚合操作，然后返回一个新的值。")]),a._v(" "),e("li",[a._v("Aggregations：sum、max、min等。")]),a._v(" "),e("li",[a._v("Distinct：返回一个数据集中去重之后的元素。")]),a._v(" "),e("li",[a._v("Join：内连接。")]),a._v(" "),e("li",[a._v("OuterJoin：外链接。")]),a._v(" "),e("li",[a._v("Cross：获取两个数据集的笛卡尔积。")]),a._v(" "),e("li",[a._v("Union：返回两个数据集的总和，数据类型需要一致。")]),a._v(" "),e("li",[a._v("First-n：获取集合中的前N个元素。")]),a._v(" "),e("li",[a._v("Sort Partition：在本地对数据集的所有分区进行排序，通过sortPartition()的链接调用来完成对多个字段的排序。")])]),a._v(" "),e("p",[a._v("Flink针对DataSet提供了一些数据分区规则，具体如下。")]),a._v(" "),e("ul",[e("li",[a._v("Rebalance：对数据集进行再平衡、重分区以及消除数据倾斜操作。")]),a._v(" "),e("li",[a._v("Hash-Partition：根据指定Key的散列值对数据集进行分区。")]),a._v(" "),e("li",[a._v("Range-Partition：根据指定的Key对数据集进行范围分区。")]),a._v(" "),e("li",[a._v("Custom Partitioning：自定义分区规则，自定义分区需要实现Partitioner接口。")])]),a._v(" "),e("h3",{attrs:{id:"sink-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#sink-2"}},[a._v("#")]),a._v(" Sink")]),a._v(" "),e("p",[a._v("Flink针对DataSet提供了大量的已经实现的Sink。")]),a._v(" "),e("ul",[e("li",[a._v("writeAsText()：将元素以字符串形式逐行写入，这些字符串通过调用每个元素的toString()方法来获取。")]),a._v(" "),e("li",[a._v("writeAsCsv()：将元组以逗号分隔写入文件中，行及字段之间的分隔是可配置的，每个字段的值来自对象的toString()方法。")]),a._v(" "),e("li",[a._v("print()：打印每个元素的toString()方法的值到标准输出或者标准错误输出流中。")])]),a._v(" "),e("h2",{attrs:{id:"flink-table-api和sql的分析及使用"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#flink-table-api和sql的分析及使用"}},[a._v("#")]),a._v(" Flink Table API和SQL的分析及使用")]),a._v(" "),e("p",[a._v("Flink针对标准的流处理和批处理提供了两种关系型API：Table API和SQL。Table API允许用户以一种很直观的方式进行select、filter和join操作；Flink SQL支持基于 Apache Calcite实现的标准SQL。针对批处理和流处理可以提供相同的处理语义和结果。")]),a._v(" "),e("p",[a._v("Flink Table API、SQL接口和Flink的DataStream API、DataSet API是紧密联系在一起的。")]),a._v(" "),e("p",[a._v("Table API和SQL是关系型API，用户可以像操作MySQL数据库表一样来操作数据，而不需要通过编写Java代码来完成Flink Function，更不需要手工为Java代码调优。另外，SQL作为一个非程序员可操作的语言，学习成本很低，如果一个系统提供SQL支持，将很容易被用户接受。")]),a._v(" "),e("h3",{attrs:{id:"table-api和sql的基本使用"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#table-api和sql的基本使用"}},[a._v("#")]),a._v(" Table API和SQL的基本使用")]),a._v(" "),e("p",[a._v("想使用Table API和SQL，首先要创建一个TableEnvironment。TableEnvironment对象是Table API和SQL集成的核心，通过TableEnvironment可以实现以下功能。")]),a._v(" "),e("ul",[e("li",[a._v("通过内部目录创建表。")]),a._v(" "),e("li",[a._v("通过外部目录创建表。")]),a._v(" "),e("li",[a._v("执行SQL查询。")]),a._v(" "),e("li",[a._v("注册一个用户自定义的Function。")]),a._v(" "),e("li",[a._v("把DataStream或者DataSet转换成Table。")]),a._v(" "),e("li",[a._v("持有ExecutionEnvironment或者StreamExecutionEnvironment的引用。")])]),a._v(" "),e("p",[a._v("一个查询中只能绑定一个指定的TableEnvironment，TableEnvironment可以通过TableEnvironment.getTableEnvironment()或者TableConfig来生成。TableConfig可以用来配置TableEnvironment或者自定义查询优化。")]),a._v(" "),e("p",[a._v("如何创建一个TableEnvironment对象?具体实现代码如下。")]),a._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("//流数据查询\nStreamExecutionEnvironment sEnv = StreamExecutionEnvironment.getExecutionEnvironment();\nStreamTableEnvironment sTableEnv = TableErnvironment.getTableEnvironment(sEnv)\n//批数据查询\nExecutionEnvironment bEnv = ExecutionEnvironment.getExecutionEnvironment()\nBatchTableEnvironment bTableEnv = TableEnviromment.getTableEnvironment(bEnv)\n")])])]),e("p",[a._v("通过获取到的TableEnvironment对象可以创建Table对象，有两种类型的Table对象：输入Table(Input Table)和输出Table(Output Table)。输入Table可以给Table API和SQL提供查询数据，输出Table可以把Table API和SQL的查询结果发送到外部存储介质中。")]),a._v(" "),e("p",[a._v("输入Table可以通过多种数据源注册。")]),a._v(" "),e("ul",[e("li",[a._v("已存在的Table对象：通常是Table API和SQL的查询结果。")]),a._v(" "),e("li",[a._v("TableSource：通过它可以访问外部数据，比如文件、数据库和消息队列。")]),a._v(" "),e("li",[a._v("DataStream或DataSet。")])]),a._v(" "),e("p",[a._v("输出Table需要使用TableSink注册。")]),a._v(" "),e("p",[a._v("下面演示如何通过TableSource注册一个Table。")]),a._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('StreamExecutionEnvironment env = StreamExecutionEnvironment.getexecutionEnvironment();\nStreamTableEnvironment tableEnv = TableEnviromment.getTableEnvironment(env)\n//创建一个TableSource\nTableSource csvSource = new CsvTableSource("/jpath/to/file", ....)\n//注册一个TableSource,称为CvsTable\ntableEnv.registerTableSource ("CsvTable", csvSopurce)\n')])])]),e("p",[a._v("接下来演示如何通过TableSink把数据写到外部存储介质中。")]),a._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('StreamExecutionEnvironment env = StreamExecutionEnvironment.getexecutionEnvironment();\nStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env)\n//创建一个TableSink\nTableSink csvSink = new CsvTableSink("/path/to/file", ...);\n//定义字段名称和类型\nString[] fieldNames = {"a", "b", "c"};\nTypeInformation[] fieldTypes = {Types.INT, Tyypes.STRING, Types.LONG}\n//注册一个Tablesink,称为CsvSinkTable\ntableEnv.registerTableSink("CsvSinkTable", fieldNames, fieldTypes, csvSink)\n')])])]),e("p",[a._v("我们知道了如何通过TableSource读取数据和通过TableSink写出数据,下面介绍如何查询Table中的数据。")]),a._v(" "),e("p",[a._v("1.使用Table API")]),a._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('StreamExecutionEnvironment env = StreamExecutiionEnvironment.getExecutionEnvironment()\nStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n//注册一个Orders表\n//通过scan操作获取到一个Table对象\nTable orders = tableEnv.scan("Orders");\n//计算所有来自法国的收入\nTable revenue = orders\n    .filter("cCountry=== \'FRANCE\'")\n    .groupBy("cID, cName")\n    .select("cID, cName, revenue.sum AS revSum");\n')])])]),e("p",[a._v("1.使用SQL")]),a._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('StreamExecutionEnvironment env = StreamExecuttionEnvironment.getExecutionEnvironment()\nStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n//注册一个Orders表\n//计算所有来自法国的收入\nTable revenue = tableEnv.sqlQuery(\n"SELECT CID, cName, SUM(revenue) AS revSum" +\n"FROM Orders " +\n"WHERE cCountry = \'FRANCE\' " +\n"GROUP BY CID, cName"\n);\n')])])]),e("h3",{attrs:{id:"datastream、dataset和table之间的转换"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#datastream、dataset和table之间的转换"}},[a._v("#")]),a._v(" DataStream、DataSet和Table之间的转换")]),a._v(" "),e("p",[a._v("Table API和SQL查询可以很容易地和DataStream、DataSet程序集成到一起。通过一个TableEnvironment，可以把DataStream或者DataSet注册为Table，这样就可以使用Table API和SQL查询了。通过TableEnvironment 也可以把Table对象转换为DataStream或者DataSet，这样就能够使用DataStream或者DataSet中的相关API了。")]),a._v(" "),e("h2",{attrs:{id:"flink支持的datatype分析"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#flink支持的datatype分析"}},[a._v("#")]),a._v(" Flink支持的DataType分析")]),a._v(" "),e("p",[a._v("Flink支持Java和Scala中的大部分数据类型。")]),a._v(" "),e("ul",[e("li",[a._v("Java Tuple和Scala Case Class。")]),a._v(" "),e("li",[a._v("Java POJO：Java实体类。")]),a._v(" "),e("li",[a._v("Primitive Type：默认支持Java和Scala基本数据类型。")]),a._v(" "),e("li",[a._v("General Class Type：默认支持大多数Java和Scala Class。")]),a._v(" "),e("li",[a._v("Hadoop Writable：支持Hadoop中实现了org.apache.Hadoop.Writable的数据类型。")]),a._v(" "),e("li",[a._v("Special Type：比如Scala中的Either Option和Try。")])]),a._v(" "),e("h2",{attrs:{id:"flink序列化分析"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#flink序列化分析"}},[a._v("#")]),a._v(" Flink序列化分析")]),a._v(" "),e("p",[a._v("Flink自带了针对诸如Int、Long和String等标准类型的序列化器。")]),a._v(" "),e("p",[a._v("如果Flink无法实现序列化的数据类型，我们可以交给Avro和Kryo。")]),a._v(" "),e("ul",[e("li",[a._v("使用Avro序列化：env.getConfig().enableForceAvro();。")]),a._v(" "),e("li",[a._v("使用Kryo序列化：env.getConfig().enableForceKryo();。")]),a._v(" "),e("li",[a._v("使用自定义序列化：env.getConfig().addDefaultKryoSerializer(Class<?> type, Class<? extendsSerializer<?>> serializerClass);。")])])])}),[],!1,null,null,null);e.default=i.exports}}]);